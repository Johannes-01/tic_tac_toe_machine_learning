# Phase 3 vs. Phase 4: Q-Table vs. Neuronales Netz

## Die zentrale Frage: Wie speichern wir Q-Werte?

### Phase 3: Q-Learning mit Tabelle (AKTUELL IMPLEMENTIERT)

**Grundprinzip:**
- Wir speichern **jede einzelne Spielsituation** in einer Hash-Map
- FÃ¼r jeden State (Spielfeld) speichern wir 9 Q-Werte (einen pro Feld)

**Beispiel:**
```java
// Q-Tabelle in QLearningAgent.java
Map<String, double[]> qTabelle = new HashMap<>();

// Speichern:
String state = "XO__X____";  // Spielfeld als String
double[] qWerte = {0.5, -0.2, 0.8, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0};
qTabelle.put(state, qWerte);

// Abrufen:
double qWertFÃ¼rFeld4 = qTabelle.get("XO__X____")[4];  // 0.3
```

**Was passiert:**
```
State: "XO__X____"  â†’  Q-Werte: [0.5, -0.2, 0.8, 0.0, 0.3, ...]
State: "XOX_X____"  â†’  Q-Werte: [0.2, 0.1, 0.9, 0.0, 0.7, ...]
State: "X___O____"  â†’  Q-Werte: [0.6, 0.0, 0.0, 0.0, 0.4, ...]
... (bis zu 5.478 States)
```

**Vorteile:**
- âœ… **Sehr einfach** - nur eine HashMap
- âœ… **Exakt** - jeder State wird individuell gelernt
- âœ… **Funktioniert perfekt fÃ¼r Tic-Tac-Toe** (nur ~5.000 States)

**Nachteile:**
- âŒ **Keine Generalisierung** - wenn wir State "XO__X____" nie gesehen haben, kennen wir keine Q-Werte dafÃ¼r
- âŒ **Nicht skalierbar** - bei Schach (10^47 States) unmÃ¶glich
- âŒ **Speicherverbrauch** - bei groÃŸen Spielen zu viel Speicher

---

### Phase 4: Deep Q-Learning mit Neuronalem Netz (OPTIONAL)

**Grundprinzip:**
- Statt jede Situation einzeln zu speichern, trainieren wir ein **Neuronales Netz**
- Das Netz lernt eine **Funktion**: `Spielfeld â†’ Q-Werte`
- Es kann Q-Werte fÃ¼r **nie gesehene** Situationen schÃ¤tzen!

**Architektur:**
```
Input Layer:          Hidden Layer:      Output Layer:
(9 Neuronen)         (18 Neuronen)      (9 Neuronen)

[Feld 1]  â†’                              [Q(Feld 1)]
[Feld 2]  â†’          [Neuron 1]  â†’       [Q(Feld 2)]
[Feld 3]  â†’          [Neuron 2]  â†’       [Q(Feld 3)]
[Feld 4]  â†’          [...]        â†’       [...]
[...]      â†’         [Neuron 18] â†’       [Q(Feld 9)]
[Feld 9]  â†’
```

**Beispiel:**

**Situation 1 (Training):**
```
Input: [1, -1, 0, 0, 1, 0, 0, 0, 0]  (X=1, O=-1, Leer=0)
       â†’ Netz berechnet â†’ [0.5, -0.2, 0.8, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0]
Target: [0.6, -0.2, 0.8, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0]  (nach Q-Update)
       â†’ Netz passt Gewichte an
```

**Situation 2 (NIE GESEHEN!):**
```
Input: [1, -1, 1, 0, 1, 0, 0, 0, 0]  (Ã¤hnlich zu Situation 1)
       â†’ Netz berechnet â†’ [0.52, -0.18, 0.75, 0.0, 0.32, ...]
```
ğŸ‘† **Das ist der Unterschied!** Das Netz kann Q-Werte fÃ¼r neue Situationen **schÃ¤tzen**!

**Was macht das Neuronale Netz?**

1. **Es lernt Muster:**
   - "Wenn ich 2 X in einer Reihe habe â†’ hoher Q-Wert fÃ¼r das 3. Feld"
   - "Wenn Gegner 2 O in einer Reihe hat â†’ hoher Q-Wert zum Blocken"

2. **Es generalisiert:**
   - Hat es gelernt, dass "XX_" gut ist...
   - ...weiÃŸ es auch, dass "_XX" gut ist (Ã¤hnliches Muster)

3. **Es komprimiert Wissen:**
   - Statt 5.000 States einzeln zu speichern...
   - ...speichert es Gewichte (z.B. 200 Werte), die alle States abdecken

**Code-Beispiel (hypothetisch fÃ¼r Phase 4):**
```java
// Statt HashMap:
// Map<String, double[]> qTabelle;

// Verwenden wir Neuronales Netz:
NeuralNetwork netz = new NeuralNetwork(9, 18, 9);  // Input, Hidden, Output

// Training:
double[] input = {1, -1, 0, 0, 1, 0, 0, 0, 0};     // Spielfeld
double[] output = netz.forward(input);              // Q-Werte berechnen
double[] target = {0.6, -0.2, 0.8, ...};           // Ziel-Q-Werte
netz.backward(target);                              // Gewichte anpassen

// Verwendung:
double[] qWerte = netz.forward(neuesSpielfeld);    // Funktioniert auch fÃ¼r nie gesehene Felder!
```

---

## Visueller Vergleich

### Phase 3: Q-Tabelle
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q-Tabelle (HashMap)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  "XO__X____" â†’ [0.5, -0.2, 0.8, 0.0, 0.3, ...]     â”‚
â”‚  "XOX_X____" â†’ [0.2, 0.1, 0.9, 0.0, 0.7, ...]      â”‚
â”‚  "X___O____" â†’ [0.6, 0.0, 0.0, 0.0, 0.4, ...]      â”‚
â”‚  ... (5.478 EintrÃ¤ge)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   LOOKUP (exakt)
         â†“
  "XO__X____" â†’ [0.5, -0.2, 0.8, ...]
```

### Phase 4: Neuronales Netz
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Neuronales Netz (Gewichte)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Gewicht[0][0] = 0.23                               â”‚
â”‚  Gewicht[0][1] = -0.45                              â”‚
â”‚  Gewicht[1][0] = 0.67                               â”‚
â”‚  ... (200 Gewichte)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   BERECHNUNG (Generalisierung)
         â†“
  [1, -1, 0, 0, 1, ...] â†’ [0.5, -0.2, 0.8, ...]
  
  Kann auch neue States berechnen:
  [1, -1, 1, 0, 1, ...] â†’ [0.52, -0.18, 0.75, ...]
           â†‘
    (nie gesehen!)
```

---

## Wann ist was sinnvoll?

### Q-Tabelle (Phase 3) ist gut fÃ¼r:
- âœ… **Kleine State-Spaces** (< 100.000 States)
- âœ… **Tic-Tac-Toe** (5.478 States)
- âœ… **Einfachheit** (schnell implementiert)
- âœ… **Garantierte Konvergenz** (findet optimale LÃ¶sung)

### Neuronales Netz (Phase 4) ist gut fÃ¼r:
- âœ… **GroÃŸe State-Spaces** (Millionen/Milliarden States)
- âœ… **Schach, Go, Atari-Spiele**
- âœ… **Generalisierung** (lernt Muster, nicht einzelne Situationen)
- âœ… **Unendliche State-Spaces** (kontinuierliche Werte)

---

## FÃ¼r Tic-Tac-Toe: Unsere Wahl

**Entscheidung: Phase 3 (Q-Tabelle) ist vÃ¶llig ausreichend!**

**Warum?**
1. Nur 5.478 States â†’ HashMap ist perfekt
2. Schneller zu implementieren
3. Garantiert optimal (keine Approximationsfehler)
4. Einfacher zu debuggen

**Neuronales Netz wÃ¤re "Overkill":**
- Wie mit einem Panzer auf eine Fliege schieÃŸen ğŸ¯
- Mehr Code, mehr KomplexitÃ¤t, mehr Bugs
- Nicht schneller, nicht besser fÃ¼r dieses Problem

**ABER:** Phase 4 ist trotzdem interessant als **LernÃ¼bung**:
- Du lernst moderne RL-Methoden
- Vorbereitung fÃ¼r komplexere Spiele
- Gut fÃ¼r die Ausarbeitung (zeigt tieferes VerstÃ¤ndnis)

---

## Zusammenfassung

| Aspekt | Phase 3 (Q-Tabelle) | Phase 4 (Neuronales Netz) |
|--------|---------------------|---------------------------|
| **Speicherung** | HashMap mit States | Gewichte im Netz |
| **Generalisierung** | Nein | Ja âœ… |
| **FÃ¼r Tic-Tac-Toe** | Perfekt âœ… | Overkill |
| **FÃ¼r Schach** | UnmÃ¶glich | Notwendig âœ… |
| **KomplexitÃ¤t** | Einfach | Komplex |
| **Framework nÃ¶tig** | Nein | Ja (Deeplearning4j) |
| **Unser Status** | âœ… Implementiert | â­ï¸ Optional |

---

## NÃ¤chste Schritte

**Empfehlung:** 
1. **Phase 5** zuerst implementieren (Training mit `trainieren()` Methode)
2. **Phase 6** danach (Model Persistence - Q-Tabelle speichern/laden)
3. **Phase 4** nur wenn Zeit Ã¼brig ist und du Deep Learning lernen willst

**Phase 4 Ã¼berspringen ist OK!** Die Aufgabe sagt "sinnvoll/erwÃ¼nscht" aber nicht "verpflichtend".

---

*Datum: 15. November 2025*  
*Status: ErklÃ¤rung - Phase 3 ist ausreichend fÃ¼r die Aufgabe!*
