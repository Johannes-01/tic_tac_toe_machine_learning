# Aufgabe 2: Best√§rkendes Lernen (Reinforcement Learning)

## Tic-Tac-Toe mit Reinforcement Learning

### Ziel der Aufgabe
Implementierung eines Tic-Tac-Toe-Spielers, der durch **Reinforcement Learning** lernt, optimal zu spielen.

---

## 1. Technische Anforderungen

### 1.1 Zu implementierende Schnittstellen
- **`ISpieler`** - Grundlegende Spielerschnittstelle
- **`ILernenderSpieler`** - Erweiterte Schnittstelle f√ºr lernende Spieler

### 1.2 Wichtige Methoden

#### Konstruktor
```java
public ReinforcementSpieler(String name, /* optionale Parameter */)
```
- Einziger zwingender Parameter: Name des Spielers
- Weitere Parameter optional m√∂glich
- Spieler muss am Ende trainiert sein
- **Best Practice**: Spieler vorher trainieren und neuronales Netz abspeichern

#### neuesSpiel(farbe, bedenkzeit)
```java
public void neuesSpiel(Farbe farbe, int bedenkzeit)
```
- Spieler muss sich die Farbe merken
- Internes Spielfeld leeren
- Parameter `bedenkzeit` (in Sekunden) wird momentan noch nicht genutzt

#### berechneZug(vorherigerZug, zeitKreis, zeitKreuz)
```java
public Zug berechneZug(Zug vorherigerZug, long zeitKreis, long zeitKreuz)
```
- `vorherigerZug`: Letzter Zug des Gegners (null beim ersten Zug)
- Gegnerischen Zug im eigenen Spielfeld vermerken
- Eigenen Zug berechnen und zur√ºckgeben
- Zeit-Parameter (in Millisekunden) enthalten momentan noch keine Werte

### 1.3 Spielablauf

1. **Spieler erzeugen** - Konstruktor aufrufen
2. **Spiel starten** - `neuesSpiel(farbe, bedenkzeit)` wird aufgerufen
3. **Z√ºge berechnen** - `berechneZug(...)` wird wiederholt aufgerufen
4. **Spielende** - Wenn keine Z√ºge mehr m√∂glich oder Gewinner feststeht

### 1.4 Bereitgestellte Komponenten

- **`Wettkampf.java`** - Testumgebung zum Trainieren und Testen
- **`tic_tac_toe.jar`** - Bibliothek mit TicTacToe-Klasse und Schnittstellen
- **Zufallsspieler** - Beispielimplementierung als Gegner nutzbar
- **`ReinforcementSpielerLeer`** - Template (nicht funktionierend)

### 1.5 Wichtige Hinweise

- ‚è±Ô∏è Zeitlimitierung muss noch NICHT ber√ºcksichtigt werden
- üß† Einsatz neuronaler Netze ist sinnvoll/erw√ºnscht
- üíæ Trainiertes Modell sollte speicherbar sein
- üéÆ Spieler ist selbst f√ºr Spielfeld-Verwaltung verantwortlich

---

## 2. Abgabeanforderungen

### 2.1 Ausarbeitung (ca. 5-10 Seiten)

#### a) Problembeschreibung
- Beschreibung des Tic-Tac-Toe Problems
- **Grobe Absch√§tzung**: Wie viele unterschiedliche Spielsituationen sind m√∂glich?

#### b) Theorieteil
- Allgemeine Einf√ºhrung in Reinforcement Learning
- Fokussierung auf das Tic-Tac-Toe Problem
- **Neuronale Netze im RL-Kontext**:
  - Einsatz von neuronalen Netzen mit RL
  - Warum/Wann ist der Einsatz sinnvoll?

#### c) L√∂sungsbeschreibung
- Verst√§ndliche Beschreibung f√ºr Kommilitonen
- **Grafischer √úberblick** (z.B. Klassendiagramm bei OOP)
- Erl√§uterung der Bestandteile des Codes, die f√ºr den Lernprozess relevant sind
- **Nachweis des Lernens**: 
  - Beispiel zeigen, dass das Programm tats√§chlich lernt
  - Z.B. Anzahl der Siege gegen Gegner vor/nach Training

#### d) Fazit
- √úberblick √ºber die L√∂sung
- Festgestellte Probleme
- M√∂gliche L√∂sungen und Erweiterungen

**Wichtig**: Quellenangaben nicht vergessen! Auch wenn Quellen RL f√ºr Tic-Tac-Toe beschreiben, m√ºssen sie angegeben werden.

### 2.2 Eclipse-Projekt
- Vollst√§ndiger Quelltext
- Kurze Anleitung zur Nutzung des Projekts

---

## 3. Implementierungsplan

### Phase 1: Grundstruktur (Vorbereitung)
- [ ] Projekt-Setup √ºberpr√ºfen
- [ ] Abh√§ngigkeiten analysieren (`tic_tac_toe.jar`)
- [ ] Schnittstellen `ISpieler` und `ILernenderSpieler` verstehen
- [ ] Grundger√ºst der `ReinforcementSpieler`-Klasse erstellen

### Phase 2: Spiellogik (Basis)
- [ ] Internes Spielfeld implementieren
- [ ] Spielzustand-Repr√§sentation definieren
- [ ] Methoden `neuesSpiel()` und `berechneZug()` implementieren (Basis)
- [ ] Validierung von Z√ºgen implementieren
- [ ] Test gegen Zufallsspieler (ohne Training)

### Phase 3: Reinforcement Learning (Kern)
- [ ] **RL-Algorithmus w√§hlen** (z.B. Q-Learning, SARSA, Deep Q-Learning)
- [ ] **State-Repr√§sentation** definieren (Spielfeld ‚Üí Eingabe)
- [ ] **Action-Space** definieren (9 m√∂gliche Z√ºge)
- [ ] **Reward-Funktion** implementieren:
  - Sieg: +1.0
  - Niederlage: -1.0
  - Unentschieden: 0.0
  - Optional: Zwischenbelohnungen
- [ ] **Exploration vs. Exploitation** (Œµ-greedy Strategie)

### Phase 4: Neuronales Netz (optional aber empfohlen)
- [ ] Neuronales Netz-Architektur definieren
  - Eingabe: 9 Felder (Spielfeld-Zustand)
  - Ausgabe: Q-Werte f√ºr 9 Aktionen
- [ ] NN-Framework w√§hlen (z.B. Deeplearning4j f√ºr Java)
- [ ] Netzwerk-Training implementieren
- [ ] Experience Replay (optional)

### Phase 5: Training
- [ ] Trainings-Loop implementieren
- [ ] Self-Play: Spieler gegen sich selbst
- [ ] Training gegen verschiedene Gegner:
  - Zufallsspieler
  - Perfekter Spieler (optional)
- [ ] Hyperparameter-Tuning:
  - Learning Rate
  - Discount Factor (Œ≥)
  - Exploration Rate (Œµ)
- [ ] Trainingsfortschritt √ºberwachen (Statistiken)

### Phase 6: Persistenz
- [ ] Modell speichern (neuronales Netz / Q-Tabelle)
- [ ] Modell laden im Konstruktor
- [ ] Trainingsmodus vs. Spielmodus unterscheiden

### Phase 7: Evaluierung
- [ ] Performance-Metriken implementieren:
  - Siegrate vor Training
  - Siegrate nach Training
  - Durchschnittliche Z√ºge pro Spiel
- [ ] Wettk√§mpfe durchf√ºhren (1000+ Spiele)
- [ ] Ergebnisse dokumentieren

### Phase 8: Dokumentation
- [ ] Ausarbeitung schreiben:
  - Problembeschreibung
  - Theorie-Teil
  - L√∂sungsbeschreibung
  - Klassendiagramm erstellen
  - Lernnachweis mit Statistiken
  - Probleme & Erweiterungen
- [ ] Code kommentieren
- [ ] README mit Nutzungsanleitung erstellen

---

## 4. Gesch√§tzte Spielsituationen

Zur Absch√§tzung der m√∂glichen Spielsituationen:

- **Obere Grenze** (alle m√∂glichen Belegungen): 3^9 = 19.683
  - Jedes der 9 Felder kann leer, X oder O sein
  
- **Realistische Anzahl** (g√ºltige Spielzust√§nde): ~5.478
  - Nach Eliminierung ung√ºltiger Zust√§nde
  - Symmetrien noch nicht ber√ºcksichtigt
  
- **Mit Symmetrien**: ~765 eindeutige Zust√§nde
  - Ber√ºcksichtigung von Rotationen und Spiegelungen

‚û°Ô∏è **Fazit**: Tic-Tac-Toe ist klein genug f√ºr Tabular RL (Q-Table), aber neuronale Netze sind trotzdem sinnvoll als Lern√ºbung und f√ºr Generalisierung.

---

## 5. RL-Algorithmen (Optionen)

### Option A: Q-Learning mit Tabelle
‚úÖ **Vorteile**: Einfach, garantierte Konvergenz  
‚ùå **Nachteile**: Nicht skalierbar, keine Generalisierung

### Option B: Deep Q-Learning (DQN)
‚úÖ **Vorteile**: Moderne Methode, Generalisierung, gute Lern√ºbung  
‚ùå **Nachteile**: Komplexer, braucht NN-Framework

### Option C: Policy Gradient / Actor-Critic
‚úÖ **Vorteile**: State-of-the-art  
‚ùå **Nachteile**: Sehr komplex f√ºr Tic-Tac-Toe

**Empfehlung**: Start mit Q-Learning + Tabelle, sp√§ter auf DQN erweitern (falls Zeit)

---

## 6. Ressourcen & Bibliotheken

### Java RL/ML Frameworks
- **Deeplearning4j** - Deep Learning f√ºr Java
- **ND4J** - Numerische Operationen (NumPy f√ºr Java)
- **Burlap** - Java RL Library

### Alternative: Python ‚Üí Java
- Python f√ºr Training (TensorFlow/PyTorch)
- Modell exportieren (ONNX)
- Java f√ºr Inferenz

---

## 7. Zeitplan (Vorschlag)

| Phase | Aufwand | Priorit√§t |
|-------|---------|-----------|
| Grundstruktur | 2-4h | Hoch |
| Spiellogik | 4-6h | Hoch |
| RL-Kern | 8-12h | Hoch |
| Neuronales Netz | 6-10h | Mittel |
| Training | 4-8h | Hoch |
| Persistenz | 2-3h | Mittel |
| Evaluierung | 3-5h | Hoch |
| Dokumentation | 8-12h | Hoch |
| **Gesamt** | **37-60h** | - |

---

## 8. Erfolgs-Kriterien

‚úÖ Spieler implementiert alle geforderten Schnittstellen  
‚úÖ Spieler kann gegen Zufallsspieler spielen  
‚úÖ Messbare Verbesserung durch Training nachweisbar  
‚úÖ Modell kann gespeichert und geladen werden  
‚úÖ Ausarbeitung vollst√§ndig und verst√§ndlich  
‚úÖ Code gut strukturiert und kommentiert  
‚úÖ Nutzungsanleitung vorhanden  

---

**Viel Erfolg bei der Implementierung! üéÆü§ñ**
